{
  "hash": "c90bb5e619b6bbbc8f0ea5f54714d14c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"M-Bias: Confounding Control Using Three Waves of Panel Data\"\nformat: html\ntitle-block-style: plain\nbibliography: references.bib\ndate: 2022-11-22\ndate-format: short\nimage: \"m-bias.png\"\nauthor: \n  - name: Joseph Bulbulia\n    orcid: 0000-0002-5861-2056\n    affiliation: Victoria University of Wellington, New Zealand\n    email: joseph.bulbulia@vuw.ac.nz\n    corresponding: yes\nkeep-tex: false\ncitation:\n  url: https://go-bayes.github.io/b-causal/\nexecute:\n  echo: false\n  warning: false\n  message: false\n  error: false\ncategories:\n  - Causal Inference\n  - Outcome-wide Science\n  - Methods\n---\n\n\n\n\n\n\n\n\n\n## Review\n\n[Elsewhere](https://go-bayes.github.io/b-causal/posts/outcomewide/outcome-wide.html), we have described our strategy for using three waves of panel data to identify causal effects. For confounding control, we adopt VanderWeele's modified disjunctive cause criterion:\n\n> control for each covariate that is a cause of the exposure, or of the outcome, or of both; exclude from this set any variable known to be an instrumental variable; and include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome [@vanderweele2020 p.441; @vanderweele2019].\n\nSuch a criterion might appear to be too liberal. It might seem that we should instead select the minimum adjustment set of confounders necessary for confounding control. Of course, the minimum adjustment set cannot generally be known. However, a liberal inclusion criterion would seem to invite confounding by over-conditioning. We next consider the risks of such liberality in three-wave panel designs.\n\n## M-bias\n\nM-bias is a form of bias that can arise when we include too many variables in our analysis, a phenomenon known as over-conditioning. Let's break this down using a concrete example. \n\nSuppose we're interested in understanding if being a perfectionist influences a person's level of humility. We start with the assumption that there's no direct cause-and-effect relationship between perfectionism (the exposure) and humility (the outcome).\n\nNow imagine we're including forgiveness in our analysis. We know that childhood schooling influences both forgiveness and perfectionism, and childhood religion affects forgiveness and humility. If we adjust for forgiveness in our analysis, an indirect path (or backdoor path) is created between perfectionism and humility, leading to M-bias. This path can be illustrated as @fig-1.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![M-bias: an example of confounding that arises from over-adjustment](m-bias_files/figure-html/fig-1-1.png){#fig-1 width=100%}\n:::\n:::\n\n\n\n\n\nBy including forgiveness in our model, we've inadvertently introduced a correlation between perfectionism and humility where one didn't previously exist. This is the essence of M-bias.\n\n## The Power of Baseline Measures\n\n\nOne might think the solution is simple - don't include forgiveness in the model. However, our understanding of causal relationships is often imperfect, and there may be plausible reasons to believe that forgiveness does, in fact, influence perfectionism. Indeed, it seems plausible that if I am more forgiving of other I will be more forgiving of my imperfections.  \n\nTo mitigate bias, we incorporate both prior measurements of the exposure and the outcome in our studies. By doing so, we control for unmeasured confounders that could bias our results. The logic is that any such bias would need to be orthogonal to the measurement of the outcome at baseline. This strategy is depicted in @fig-2.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![M-bias: confounding control by including previous measures of the outcome](m-bias_files/figure-html/fig-2-1.png){#fig-2 width=100%}\n:::\n:::\n\n\n\n\n\n\n## Conclusion\n\nIn our pursuit of understanding causal relationships, we must carefully navigate the risk of M-biasâ€”a form of confounding that can emerge from over-adjusting for variables. We've outlined a strategy to mitigate this bias by including both prior measurements of the exposure and the outcome in our studies. This approach provides a robust mechanism to control for unmeasured confounders that might otherwise skew our results. However, even with these measures, we cannot guarantee the elimination of all confounding. For this reason, we also conduct sensitivity analysis using E-values to assess the robustness of our findings to potential unmeasured confounding. E-values provide a quantitative measure of the minimum strength an unmeasured confounder would need to fully explain away an observed association. \n\nIn future posts, we will delve more deeply into the concept of E-values and their role in robust causal inference. By leveraging strategies for confounding control that include previous measures of the outcome and exposure variables, as well as senstivity analysis, we strive for more reliable, accurate insights in our studies.\n\n\n\n## Acknowledgements\n\nI am grateful to Templeton Religion Trust Grant 0418 for supporting my work.[^2]\n\n[^2]: The funders played no role in the design or interpretation of this research.\n",
    "supporting": [
      "m-bias_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}